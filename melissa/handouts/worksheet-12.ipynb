{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANIPULATING TABULAR DATA\n",
    "\n",
    "https://cyberhelp.sesync.org/census-data-manipulation-in-R-lesson/\n",
    "## Lesson Objectives\n",
    "\n",
    "* Review what makes a dataset tidy.\n",
    "* Meet a complete set of functions for most table manipulations.\n",
    "* Learn to transform datasets with split-apply-combine procedures.\n",
    "* Understand the basic join operation.\n",
    "\n",
    "## Specific Achievements\n",
    "\n",
    "* Reshape data frames with pandas\n",
    "* Summarize data by groups with pandas\n",
    "* Combine multiple data frame operations with pipes\n",
    "* Combine multiple data frames with “joins” (merge)\n",
    "\n",
    "Data frames occupy a central place in Python data analysis pipelines. The panda package provide the objects and most necessary tools to subset, reformat and transform data frames. The key functions in both packages have close counterparts in SQL (Structured Query Language), which provides the added bonus of facilitating translation between python and relational databases.\n",
    "\n",
    "\n",
    "## Tidy Concept\n",
    "Most time is spent on cleaning and wrangling data rather than analysis. In 2014, Hadley Wickam (R developer at RStudio) published a paper that defines the concepts underlying tidy datasets. Hadley Wickam defined tidy datasets as those where:\n",
    "\n",
    "* each variable forms a column (also called field)\n",
    "* each observation forms a row\n",
    "* each type of observational unit forms a table\n",
    "\n",
    "These guidelines may be familiar to some of you—they closely map to best practices for “normalization” in database design.It correspond to the 3rd normal form’s described by Codd 1990 but uses the language of statical analysis rather than relationtional database.\n",
    "\n",
    "Consider a data set where the outcome of an experiment has been recorded in a perfectly appropriate way:\n",
    "\n",
    "bloc\tdrug\tcontrol\tplacebo\n",
    "1\t0.22\t0.58\t0.31\n",
    "2\t0.12\t0.98\t0.47\n",
    "3\t0.42\t0.19\t0.40\n",
    "The response data are present in a compact matrix, as you might record it on a spreadsheet. The form does not match how we think about a statistical model, such as:\n",
    "\n",
    "response ~ block + treatment\n",
    "\n",
    "In a tidy format, each row is a complete observation: it includes the response value and all the predictor values. In this data, some of those predictor values are column headers, so the table needs to be reshaped. The pandas package provides functions to help re-organize tables.\n",
    "\n",
    "The third principle of tidy data, one table per category of observed entities, becomes especially important in synthesis research. Following this principle requires holding tidy data in multiple tables, with associations between them formalized in metadata, as in a relational database.\n",
    "\n",
    "Datasets split across multiple tables are unavoidable in synthesis research, and commonly used in the following two ways (often in combination):\n",
    "\n",
    "* two tables are “un-tidied” by joins, or merging them into one table\n",
    "* statistical models conform to the data model through a hierarchical structure or employing “random effects”\n",
    "\n",
    "The pandas package includes several functions that all perform variations on table joins needed to “un-tidy” your tables, but there are only two basic types of table relationships to recognize:\n",
    "\n",
    "* One-to-one relationships allow tables to be combined based on the same unique identifier (or “primary key”) in both tables.\n",
    "* Many-to-one relationships require non-unique “foreign keys” in the first table to match the primary key of the second.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worksheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide to long\n",
    "\n",
    "The pandas package’s melt function reshapes “wide” data frames into “long” ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "trial_df = pd.DataFrame({\"block\": [1,2,3],\n",
    "              \"drug\": [0.22,0.12,0.42],\n",
    "              \"control\": [0.58,0.98,0.19],\n",
    "              \"placebo\": [0.31,0.47,0.40]})\n",
    "trial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_trial_df = pd.melt(trial_df,\n",
    "                  id_vars=['block'],\n",
    "                  var_name='treatment',\n",
    "                  value_name='response')\n",
    "tidy_trial_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns, accept for “block”, are stacked in two columns: a “key” and a “value”. The key column gets the name treatment and the value column receives the name response. For each row in the result, the key is taken from the name of the column and the value from the data in the column.\n",
    "\n",
    "## Long to wide\n",
    "\n",
    "Data can also fail to be tidy when a table is too long. The Entity-Attribute-Value (EAV) structure common in large databases distributes multiple attributes of a single entity/observation into separate rows.\n",
    "\n",
    "Remember that the exact state of “tidy” may depend on the analysis: the key is knowing what counts as a complete observation. For example, the community ecology package vegan requires a matrix of species counts, where rows correspond to species and columns to sites. This may seem like too “wide” a format, but in the packages several multi-variate analyses, the abundance of a species across multiple sites is considered a complete observation.\n",
    "\n",
    "Consider survey data on participant’s age and income stored in a EAV structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the \"pivot\" function to go from long format to wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = tidy_trial_df.pivot(index='block',\n",
    "                          columns='treatment',\n",
    "                          values='response')\n",
    "df2 = df2.reset_index()\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider survey data on participant’s age and income stored in a EAV structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO, BytesIO\n",
    "\n",
    "text_string = StringIO(\"\"\"\n",
    "participant,attr,val\n",
    "1,age,24\n",
    "2,age,57\n",
    "3,age,13\n",
    "1,income,30\n",
    "2,income,60\n",
    "\"\"\")\n",
    "\n",
    "survey_df = pd.read_csv(text_string, sep=\",\")\n",
    "survey_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data with the pivot function, which “reverses” a melt. These are equivalent to spread and gather in the dplyr r package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_survey = survey_df.pivot(index='participant',\n",
    "                          columns='attr',\n",
    "                          values='val')\n",
    "print(tidy_survey.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_survey = tidy_survey.reset_index()\n",
    "tidy_survey.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_survey.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"reset_index\" adds the index as a column. It generates a new inde starting from 0 to the number of rows minus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "To learn about data transformation with pandas, we need more data. The Census Bureau collects subnational economic data for the U.S., releasing annual County Business Patterns (CBP) datasets including the number of establishments, employment, and payroll by industry. They also conduct the American Community Survey (ACS) and publish, among other demographic and economic variables, estimates of median income for individuals working in different industries.\n",
    "\n",
    "* County Business Patterns (CBP)\n",
    "* American Community Survey (ACS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cbp = pd.read_csv('data/cbp15co.csv')\n",
    "cbp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cbp.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the CBP dataset documentation for an explanation of the variables we don’t discuss in this lesson.\n",
    "\n",
    "Modify the import to clean up this read: consider the data type for FIPS codes along with what string in this CSV file represents NAs, a.k.a. data that is not-available or missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cbp = pd.read_csv(\n",
    "  'data/cbp15co.csv',\n",
    "  na_values = \"NULL\",\n",
    "  keep_default_na=False,\n",
    "  dtype =  {\"FIPSTATE\": np.str, \n",
    "  \"FIPSCTY\": np.str}\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What changed?\n",
    "### Answer\n",
    "Using dtypes() shows that the character string \"\" in the CSV file is no longer read into R as missing data (an NA) but as an empty string. The two named “FIPS” columns are now correctly read as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "acs =  pd.read_csv(\n",
    "  'data/ACS/sector_ACS_15_5YR_S2413.csv',\n",
    "  dtype = {\"FIPS\": np.str}\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s display the data types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acs.dtypes\n",
    "print(acs.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two datasets both contain economic variables for each U.S. county and specified by different categories of industry. The data could potentially be manipulated into a single table reflecting the follow statistical model.\n",
    "\n",
    "median_income ~ industry + establishment_size\n",
    "\n",
    "### Key Functions\n",
    "Function\tReturns\n",
    "query\tkeep rows that satisfy conditions\n",
    "assign\tapply a transformation to existing [split] columns\n",
    "['col1', 'col2']\tselect and keep columns with matching names\n",
    "merge\tmerge columns from separate tables into one table\n",
    "groupby\tsplit data into groups by an existing factor\n",
    "agg\tsummarize across rows to use after groupby [and combine split groups]\n",
    "The table above summarizes the most commonly used functions in pandas, which we will demonstrate in turn on data from the U.S. Census Bureau.\n",
    "\n",
    "## Typical Data Manipulation Functions\n",
    "\n",
    "## Filter Pattern matching\n",
    "The cbp table includes character NAICS column. Of the 2 million observations, lets see how many observations are left when we keep only the 2-digit NAICS codes, representing high-level sectors of the economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import pandas as pd\n",
    "cbp2 = cbp[cbp['NAICS'].str.contains(\"----\")]\n",
    "cbp2 = cbp2[~cbp2.NAICS.str.contains(\"-----\")]\n",
    "cbp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a logical we used the function contains from pandas to filter the dataset in two steps. The function contains allows for pattern matching of any character within strings. The ~ is used to remove the rows that contains specific patterns.\n",
    "\n",
    "Filtering string often uses pattern matching by regular expressions which may be a bit more manageable, and streamlines the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp3 = cbp[cbp['NAICS'].str.contains('[0-9]{2}----')]\n",
    "cbp3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering, updating and transforming columns\n",
    "The assign function is the pandas answer to updating or altering your columns. It performs arbitrary operations on existing columns and appends the result as a new column of the same length.\n",
    "\n",
    "Here are two ways to create a new column using assign and the [ ] operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp3[\"FIPS\"] = cbp3[\"FIPSTATE\"]+cbp3[\"FIPSCTY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp3.assign(FIPS2=lambda x: x['FIPSTATE']+x['FIPSCTY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select\n",
    "To keep particular columns of a data frame (rather than filtering rows), use the filter or [ ] functions with arguments that match column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to “match” is by including complete names, each one you want to keep:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp3 = cbp3[['FIPS','NAICS','N1_4', 'N5_9', 'N10_19']] \n",
    "cbp3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the filter function to select all columns starting with N or matching with ‘FIPS’ or ‘NAICS’ pattern. The filter command is useful when chaining methods (or piping operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp4= cbp.filter(regex='^N|FIPS|NAICS',axis=1) \n",
    "cbp4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join\n",
    "The CBP dataset uses FIPS to identify U.S. counties and NAICS codes to identify types of industry. The ACS dataset also uses FIPS but their data may aggregate across multiple NAICS codes representing a single industry sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sector =  pd.read_csv(\n",
    "  'data/ACS/sector_naics.csv',\n",
    "  dtype = {\"NAICS\": np.int64})\n",
    "print(sector.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cbp.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp.head()\n",
    "cbp.dtypes\n",
    "cbp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sector.dtypes)\n",
    "print(sector.shape) #24 economic sectors\n",
    "sector.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the primary challenge in combining secondary datasets for synthesis research is dealing with their different sampling frames. A very common issue is that data are collected at different “scales”, with one dataset being at higher spatial or temporal resolution than another. The differences between the CBP and ACS categories of industry present a similar problem, and require the same solution of re-aggregating data at the “lower resolution”.\n",
    "\n",
    "## Many-to-One\n",
    "Before performing the join operation, some preprocessing is necessary to extract from the NAICS columns the first two digits matching the sector identifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logical_idx = cbp['NAICS'].str.match('[0-9]{2}----') #boolean index\n",
    "cbp = cbp.loc[logical_idx]\n",
    "cbp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp['NAICS']= cbp.NAICS.apply(lambda x: np.int64(x[0:2])) # select first two digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Many to one to join economic sector code to NAICS\n",
    "\n",
    "cbp_test = cbp.merge(sector, on = \"NAICS\", how='inner')\n",
    "cbp_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cbp_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NAICS field in the cbp table can have the same value multiple times, it is not a primary key in this table. In the sector table, the NAICS field is the primary key uniquely identifying each record. The type of relationship between these tables is therefore “many-to-one”.\n",
    "\n",
    "### Question\n",
    "Note that we lost a couple thousand rows through this join. How could cbp have fewer rows after a join on NAICS codes?\n",
    "### Answer\n",
    "The CBP data contains an NAICS code not mapped to a sector—the “error code” 99 is not present in sector. The use of “error codes” that could easilly be mistaken for data is frowned upon.\n",
    "\n",
    "## Group By\n",
    "A very common data manipulation procedure know as “split-apply-combine” tackles the problem of applying the same transformation to subsets of data while keeping the result all together. We need the total number of establishments in each size class aggregated within each county and industry sector.\n",
    "\n",
    "The pandas function groupby begins the process by indicating how the data frame should be split into subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cbp[\"FIPS\"] = cbp[\"FIPSTATE\"]+cbp[\"FIPSCTY\"]\n",
    "cbp = cbp.merge(sector, on = \"NAICS\")\n",
    "\n",
    "cbp_grouped = cbp.groupby(['FIPS','Sector'])\n",
    "cbp_grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, nothing has really changed:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp_grouped.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The groupby statement generates a groupby data frame. You can add multiple variables (separated by commas) in groupby; each distinct combination of values across these columns defines a different group.\n",
    "\n",
    "## Summarize\n",
    "\n",
    "The operation to perform on each group is summing: we need to sum the number of establishments in each group. Using pandas functions, the summaries are automically combined into a data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped_df = (cbp\n",
    ".groupby(['FIPS', 'Sector']) \n",
    ".agg('sum')\n",
    ".filter(regex='^N')\n",
    ".drop(columns=['NAICS'])\n",
    ")\n",
    "\n",
    "grouped_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “combine” part of “split-apply-combine” occurs automatically, when the attributes introduced by groupby are dropped. You can see attributes by running the dtypes function on the data frame.\n",
    "\n",
    "There is now a one-to-one relationship between cbp and acs, based on the combination of FIPS and Sector as the primary key for both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_cbp = grouped_df.merge(acs,on='FIPS',)\n",
    "print(acs_cbp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_cbp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, however, the one-to-one relationship does not mean all rows are preserved by the join. The specific nature of the inner_join is to keep all rows, even duplicating rows if the relationship is many-to-one, where there are matching values in both tables, and discarding the rest.\n",
    "\n",
    "The acs_cbp table now includes the median_income variable from the ACS and appropriatey aggregated establishment size information (the number of establishments by employee bins) from the CBP table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_cbp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "The following cheat sheets and tutorials repeat much of this lesson, but also provide information on additional functions for “data wrangling”.\n",
    "\n",
    "* Data Wrangling Cheat Sheet\n",
    "* Tidyverse In Pandas\n",
    "* String and Text With Pandas\n",
    "\n",
    "The first is a set of cheat sheets created by pydata.org, and provides a handy, visual summary of all the key functions discussed in this lesson. It also lists some of the auxiliary functions that can be used within each type of expression, e.g. aggregation functions for summarize, “moving window” functions for mutate, etc. For those familiar with the tidyverse univers, please consult the second link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_cbp.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
