---
title: "SampleData"
output: html_notebook
---


```{r}
#load libraries
library(stringr)
library(data.table)
library(dplyr)
library(magrittr)
library(NLP)
library(tidytext)
library(tm)
library(topicmodels)
library(ggplot2)

#load data
pq_metadata <- fread('Data/02_Working/pq_metadata.csv')

# displays column names
names(pq_metadata)

```

```{r}
# Head displays the first 6 rows of the data.table
head(pq_metadata)

```

```{r}
# what are the unique publication titles 
unique(pq_metadata$`Publication title`)

```
```{r}

# How do you want to treat NA?
# subset the data by the NAs, explore them & their origin
# can we programmatically fix it, or do we have to do it manually?

sum(is.na(pq_metadata$`Publication title`)) ### there are 45 rows that have NAs

pqNA<-pq_metadata[is.na(pq_metadata$`Publication title`),]
##lots of info missing from other fields in this group... remove? 

sum(is.na(pqNA$`Full text`)) ## there are 35 rows without text among those without titles


```


```{r}
# Substitute 
# "Bangor Daily News; Bang or, Me."  
# "Bangor Daily News; Ba ngor, Me." 
# "Bangor Dail y News; Bangor, Me."  
# with "Bangor Daily News; Bangor, Me."  

# "Morning Sentinel; Wate rville, Me." 
# "Central Maine Morning Sentinel; Waterville, Me."
#  with "Morning Sentinel; Waterville, Me."       

# "Kennebec Journal; Augusta, Me." 
# "Kennebec Journal; Augusta, Me ."


#pq_metaSub<-pq_metadata #create new data frame to avoid saving over

pq_metaSub <- pq_metadata %>% 
  mutate(`Publication title` = recode(`Publication title`,
                                      'Bangor Daily News; Bang or, Me.' = 'Bangor Daily News; Bangor, Me.',
                                      'Bangor Daily News; Ba ngor, Me.' = 'Bangor Daily News; Bangor, Me.',
                                      'Bangor Dail y News; Bangor, Me.' = 'Bangor Daily News; Bangor, Me.',
                                      'Morning Sentinel; Wate rville, Me.' = 'Morning Sentinel; Waterville, Me.',
                                      'Central Maine Morning Sentinel; Waterville, Me.' = 'Morning Sentinel; Waterville, Me.',
                                      'Kennebec Journal; Augusta, Me .' = 'Kennebec Journal; Augusta, Me.',
                                      'Portland Press Herald; Port land, Me.' = 'Portland Press Herald; Portland, Me.'))

#check to see if pub title substitution worked --- yes, no weird repeats
unique(pq_metaSub$`Publication title`)

```




```{r}

fulltext_df<-pq_metaSub %>%
  group_by(`Publication title`) %>%
  summarise(agg_full_text = toString(`Full text`), .groups = 'keep')

```

```{r}

#fulltext_df

```

# Predefined Cleaning Functions
# https://cyberhelp.sesync.org/text-mining-lesson/course/

```{r}

#create full text object for each publication

BDN<-fulltext_df$agg_full_text[1]

KJ<-fulltext_df$agg_full_text[2]

MT<-fulltext_df$agg_full_text[3]

MS<-fulltext_df$agg_full_text[4]

PPH<-fulltext_df$agg_full_text[5]

SJ<-fulltext_df$agg_full_text[6]

# now have to make compatible with a VCorpus object
fullcorp_vcorpus <- Corpus(VectorSource(fulltext_df$agg_full_text))
BDN_vcorpus <- Corpus(VectorSource(BDN)) 
KJ_vcorpus<- Corpus(VectorSource(KJ)) 
MT_vcorpus<- Corpus(VectorSource(MT)) 
MS_vcorpus<- Corpus(VectorSource(MS)) 
PPH_vcorpus<- Corpus(VectorSource(PPH)) 
SJ_vcorpus<- Corpus(VectorSource(SJ)) 


```



```{r}
# table ngrams for now - processing is really slow
# python is faster
#NLP_tokenizer <- function(x) {
#  unlist(lapply(ngrams(words(x), 1:2), paste, collapse = "_"), use.names = FALSE)
#}

```

```{r}

#tokenize = NLP_tokenizer,

# stemming: dropping -ing -s -es
# lemmatization -- where is this?
control_list_ngram = list(removePunctuation = FALSE,
                          removeNumbers = FALSE, 
                          stopwords = stopwords("english"), 
                          tolower = T, 
                          stemming = T, 
                          weighting = function(x)
                            weightTf(x)
                          )

```

# Document-Term Matrix

```{r}
full_DocTermMatrix <- DocumentTermMatrix(fullcorp_vcorpus, control_list_ngram)

KJ_DocTermMatrix <- DocumentTermMatrix(KJ_vcorpus, control_list_ngram)

BDN_DocTermMatrix <- DocumentTermMatrix(BDN_vcorpus, control_list_ngram)

MT_DocTermMatrix <- DocumentTermMatrix(MT_vcorpus, control_list_ngram)

MS_DocTermMatrix <- DocumentTermMatrix(MS_vcorpus, control_list_ngram)

PPH_DocTermMatrix <- DocumentTermMatrix(PPH_vcorpus, control_list_ngram)

SJ_DocTermMatrix <- DocumentTermMatrix(SJ_vcorpus, control_list_ngram)


```
```{r}

# make a list of the documents to iterate through
DTM_list<-list(full_DocTermMatrix, KJ_DocTermMatrix, BDN_DocTermMatrix,
            MT_DocTermMatrix, MS_DocTermMatrix, PPH_DocTermMatrix,
            SJ_DocTermMatrix)

DTM_names <- c('full_DocTermMatrix', 'KJ_DocTermMatrix', 'BDN_DocTermMatrix',
            'MT_DocTermMatrix', 'MS_DocTermMatrix', 'PPH_DocTermMatrix',
            'SJ_DocTermMatrix')
```


## From here downward the code needs to be changed from the examples to use our own corpus

```{r}

hist_words <- function(DTM, plotTitle='') {
  # The tidytext package converts the (wide) Document Term Matrix into a longer form table with a row for every document and term combination.
  DTM_terms<- tidy(DTM)
  
  # The words data frame is more amenable to further inspection and cleaning, such as removing outliers.
  # frequency of word letter lengths -- if there are a lot of long words or short words, probably need to be dropped.
  DTM_words <- DTM_terms %>%
    group_by(term) %>%
    summarise(
      n = n(),
      total = sum(count)) %>%
    mutate(nchar = nchar(term))
  
  ggplot(DTM_words, aes(x = nchar)) +
    ggtitle(plotTitle) +
    theme(plot.title = element_text(hjust = 0.5)) +
    geom_histogram(binwidth = 1)
}

```

```{r}

# iterate through hist_words function
for (i in 1:length(DTM_list)) {
  
  print(hist_words(DTM_list[[i]], DTM_names[i]))
}


```



```{r}
dtt_trimmed <- words %>%
  filter(
    nchar < 16,
    n > 1,
    total > 3) %>%
  select(term) %>%
  inner_join(dtt)

```

```{r}
dtm_trimmed <- dtt_trimmed %>%
  cast_dtm(document, term, count)

```

```{r}
# console
dtm_trimmed

```

## Term Correlations


```{r}
word_assoc <- findAssocs(dtm_trimmed, 'ken', 0.6)
word_assoc <- data.frame(
  word = names(word_assoc[[1]]),
  assoc = word_assoc,
  row.names = NULL)

```

### word cloud

```{r}
library(ggwordcloud)

ggplot(word_assoc,
  aes(label = word, size = ken)) +
  geom_text_wordcloud_area()

```


# Topic Modelling

## LDA
```{r}
library(topicmodels)

seed = 12345
fit = LDA(BDN_DocTermMatrix, k = 5, control = list(seed=seed))
```


```{r}
terms(fit, 20)

```


```{r}
email_topics <- as.data.frame(
  posterior(fit, dtm_trimmed)$topics)

```

```{r}
# console
head(email_topics)

```


```{r}
library(ggwordcloud)

topics <- tidy(fit) %>%
  filter(beta > 0.004)

ggplot(topics,
  aes(size = beta, label = term)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  facet_wrap(vars(topic))
```