---
title: "SampleData"
output: html_notebook
---


```{r}
#load libraries
library(stringr)
library(data.table)
library(dplyr)
library(magrittr)
library(NLP)
library(tidytext)
library(tm)
library(topicmodels)

#load data
pq_metadata <- fread('Data/02_Working/pq_metadata.csv')

# displays column names
names(pq_metadata)

```

```{r}
# Head displays the first 6 rows of the data.table
head(pq_metadata)

```

```{r}
# what are the unique publication titles 
unique(pq_metadata$`Publication title`)

```

```{r}
# Substitute 
# "Bangor Daily News; Bang or, Me."  
# "Bangor Daily News; Ba ngor, Me." 
# "Bangor Dail y News; Bangor, Me."  
# with "Bangor Daily News; Bangor, Me."  

# "Morning Sentinel; Wate rville, Me." 
# "Central Maine Morning Sentinel; Waterville, Me."
#  with "Morning Sentinel; Waterville, Me."       

# "Kennebec Journal; Augusta, Me." 
# "Kennebec Journal; Augusta, Me ."

pq_metaSub<-pq_metadata #create new data frame to avoid saving over

pq_metaSub$`Publication title` <- as.character(pq_metaSub$`Publication title`)

pq_metaSub[pq_metaSub$`Publication title` == 
              "Bangor Daily News; Bang or, Me."] <- "Bangor Daily News; Bangor, Me."
pq_metaSub[pq_metaSub$`Publication title` == 
              "Bangor Daily News; Ba ngor, Me."] <- "Bangor Daily News; Bangor, Me."
pq_metaSub[pq_metaSub$`Publication title` == 
              "Bangor Dail y News; Bangor, Me." ] <- "Bangor Daily News; Bangor, Me."
pq_metaSub[pq_metaSub$`Publication title` == 
              "Morning Sentinel; Wate rville, Me." ] <- "Morning Sentinel; Waterville, Me."
pq_metaSub[pq_metaSub$`Publication title` == 
              "Central Maine Morning Sentinel; Waterville, Me." ] <- "Morning Sentinel; Waterville, Me."
pq_metaSub[pq_metaSub$`Publication title` == 
              "Kennebec Journal; Augusta, Me ." ] <- "Kennebec Journal; Augusta, Me."
pq_metaSub[pq_metaSub$`Publication title` == 
             "Portland Press Herald; Port land, Me."] <- "Portland Press Herald; Portland, Me."

#check to see if pub title substitution worked --- yes, no weird repeats
unique(pq_metaSub$`Publication title`)
```


```{r}

# How do you want to treat NA?
# subset the data by the NAs, explore them & their origin
# can we programmatically fix it, or do we have to do it manually?

sum(is.na(pq_metaSub$`Publication title`)) ### there are 45 rows that have NAs

pqNA<-pq_metaSub[is.na(pq_metaSub$`Publication title`),]
##lots of info missing from other fields in this group... remove? 

sum(is.na(pqNA$`Full text`)) ## there are 35 rows without text among those without titles


```

```{r}

fulltext_df<-pq_metaSub %>%
  group_by(`Publication title`) %>%
  summarise(agg_full_text = toString(`Full text`), .groups = 'keep')

```

```{r}

#fulltext_df

```

# Predefined Cleaning Functions
# https://cyberhelp.sesync.org/text-mining-lesson/course/

```{r}

#create full text object for each publication

BDN<-fulltext_df$agg_full_text[1]

KJ<-fulltext_df$agg_full_text[2]

MT<-fulltext_df$agg_full_text[3]

MS<-fulltext_df$agg_full_text[4]

PPH<-fulltext_df$agg_full_text[5]

SJ<-fulltext_df$agg_full_text[6]

# now have to make compatible with a VCorpus object
BDN_vcorpus <- Corpus(VectorSource(BDN)) 
KJ_vcorpus<- Corpus(VectorSource(KJ)) 
MT_vcorpus<- Corpus(VectorSource(MT)) 
MS_vcorpus<- Corpus(VectorSource(MS)) 
PPH_vcorpus<- Corpus(VectorSource(PPH)) 
SJ_vcorpus<- Corpus(VectorSource(SJ)) 

```



```{r}
# table ngrams for now - processing is really slow
#NLP_tokenizer <- function(x) {
#  unlist(lapply(ngrams(words(x), 1:2), paste, collapse = "_"), use.names = FALSE)
#}

```

```{r}

#tokenize = NLP_tokenizer,

control_list_ngram = list(removePunctuation = FALSE,
                          removeNumbers = FALSE, 
                          stopwords = stopwords("english"), 
                          tolower = T, 
                          stemming = T, 
                          weighting = function(x)
                            weightTf(x)
                          )

```

# Document-Term Matrix

```{r}
KJ_DocTermMatrix <- DocumentTermMatrix(KJ_vcorpus, control_list_ngram)

BDN_DocTermMatrix <- DocumentTermMatrix(BDN_vcorpus, control_list_ngram)

MT_DocTermMatrix <- DocumentTermMatrix(MT_vcorpus, control_list_ngram)

MS_DocTermMatrix <- DocumentTermMatrix(MS_vcorpus, control_list_ngram)

PPH_DocTermMatrix <- DocumentTermMatrix(PPH_vcorpus, control_list_ngram)

SJ_DocTermMatrix <- DocumentTermMatrix(SJ_vcorpus, control_list_ngram)

```


## From here downward the code needs to be changed from the examples to use our own corpus

```{r}

library(tidytext)
library(dplyr)

BDN_terms<- tidy(BDN_DocTermMatrix)
BDN_words <- BDN_terms %>%
  group_by(term) %>%
  summarise(
    n = n(),
    total = sum(count)) %>%
  mutate(nchar = nchar(term))


```

```{r}

library(ggplot2)

ggplot(BDN_words, aes(x = nchar)) +
  geom_histogram(binwidth = 1)

```

```{r}
dtt_trimmed <- words %>%
  filter(
    nchar < 16,
    n > 1,
    total > 3) %>%
  select(term) %>%
  inner_join(dtt)

```

```{r}
dtm_trimmed <- dtt_trimmed %>%
  cast_dtm(document, term, count)

```

```{r}
# console
dtm_trimmed

```

## Term Correlations


```{r}
word_assoc <- findAssocs(dtm_trimmed, 'ken', 0.6)
word_assoc <- data.frame(
  word = names(word_assoc[[1]]),
  assoc = word_assoc,
  row.names = NULL)

```

### word cloud

```{r}
library(ggwordcloud)

ggplot(word_assoc,
  aes(label = word, size = ken)) +
  geom_text_wordcloud_area()

```


# Topic Modelling

## LDA
```{r}
library(topicmodels)

seed = 12345
fit = LDA(BDN_DocTermMatrix, k = 5, control = list(seed=seed))
```


```{r}
terms(fit, 20)

```


```{r}
email_topics <- as.data.frame(
  posterior(fit, dtm_trimmed)$topics)

```

```{r}
# console
head(email_topics)

```


```{r}
library(ggwordcloud)

topics <- tidy(fit) %>%
  filter(beta > 0.004)

ggplot(topics,
  aes(size = beta, label = term)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  facet_wrap(vars(topic))
```