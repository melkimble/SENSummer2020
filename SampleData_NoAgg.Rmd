---
title: "SampleData_NoAgg"
output: html_notebook
---

```{r}
#install.packages("webshot")

#install.packages('method')
#load libraries
library(stringr)
library(data.table)
library(dplyr)
library(tidyr)
library(magrittr)
library(NLP)
library(tidytext)
library(tm)
library(topicmodels)
library(ggplot2)
library(scales)


# https://rstudio-pubs-static.s3.amazonaws.com/266040_d2920f956b9d4bd296e6464a5ccc92a1.html
library(fpc)   
library(wordcloud)
library(cluster)
library(stringi)
library(proxy)
library(RTextTools)

# to save wordclouds
library(webshot)


# https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html
#library(methods)
#library(quanteda)
#library(broom)

#library(gtable)
#library(grid)
#library(gridExtra)


#load data
pq_metadata <- fread('Data/02_Working/pq_metadata.csv')

# displays column names
names(pq_metadata)

```

```{r}
# Head displays the first 6 rows of the data.table
head(pq_metadata)

```

```{r}
# what are the unique publication titles 
unique(pq_metadata$`Publication title`)

```
```{r}

# How do you want to treat NA?
# subset the data by the NAs, explore them & their origin
# can we programmatically fix it, or do we have to do it manually?

sum(is.na(pq_metadata$`Publication title`)) ### there are 45 rows that have NAs

pqNA<-pq_metadata[is.na(pq_metadata$`Publication title`),]
##lots of info missing from other fields in this group... remove? 

sum(is.na(pqNA$`Full text`)) ## there are 35 rows without text among those without titles


```


```{r}
# Substitute 
# "Bangor Daily News; Bang or, Me."  
# "Bangor Daily News; Ba ngor, Me." 
# "Bangor Dail y News; Bangor, Me."  
# with "Bangor Daily News; Bangor, Me."  

# "Morning Sentinel; Wate rville, Me." 
# "Central Maine Morning Sentinel; Waterville, Me."
#  with "Morning Sentinel; Waterville, Me."       

# "Kennebec Journal; Augusta, Me." 
# "Kennebec Journal; Augusta, Me ."


#pq_metaSub<-pq_metadata #create new data frame to avoid saving over

pq_metaSub <- pq_metadata %>% 
  tidyr::drop_na(`Publication title`) %>%
  mutate(`Publication title` = recode(`Publication title`,
                                      'Bangor Daily News; Bang or, Me.' = 'Bangor Daily News; Bangor, Me.',
                                      'Bangor Daily News; Ba ngor, Me.' = 'Bangor Daily News; Bangor, Me.',
                                      'Bangor Dail y News; Bangor, Me.' = 'Bangor Daily News; Bangor, Me.',
                                      'Morning Sentinel; Wate rville, Me.' = 'Morning Sentinel; Waterville, Me.',
                                      'Central Maine Morning Sentinel; Waterville, Me.' = 'Morning Sentinel; Waterville, Me.',
                                      'Kennebec Journal; Augusta, Me .' = 'Kennebec Journal; Augusta, Me.',
                                      'Portland Press Herald; Port land, Me.' = 'Portland Press Herald; Portland, Me.'))

#check to see if pub title substitution worked --- yes, no weird repeats
unique(pq_metaSub$`Publication title`)

```




```{r}

fulltext_df<-pq_metaSub %>%
  group_by(`Publication title`) %>%
  summarise(agg_full_text = toString(`Full text`), .groups = 'keep')
```

```{r}



#fulltext_qcorp <- quanteda::corpus(fulltext_df, text_field = "agg_full_text") # you have to tell it the name of the text field
#fulltext_dfm <- quanteda::dfm(fulltext_qcorp, verbose = FALSE)

#fulltext_tidy <- tidy(fulltext_dfm)
# lots of options to dfm read the help pages
#fulltext_df

```

# Predefined Cleaning Functions
# https://cyberhelp.sesync.org/text-mining-lesson/course/



```{r}
# table ngrams for now - processing is really slow
# python is faster
#NLP_tokenizer <- function(x) {
#  unlist(lapply(ngrams(words(x), 1:2), paste, collapse = "_"), use.names = FALSE)
#}

#prints stopwords for english
stopwords(kind="SMART")
```






```{r}

```


## From here downward the code needs to be changed from the examples to use our own corpus

```{r}

tidy_words <- function(DTM, tidyTrim=FALSE) {
  ret_list <- list()
  
  # The tidytext package converts the (wide) Document Term Matrix into a longer form table with a row for every document and term combination.
  DTM_terms<- tidy(DTM)
  ret_list[['DTM_terms']]<-DTM_terms
  
  
  # The words data frame is more amenable to further inspection and cleaning, such as removing outliers.
  # frequency of word letter lengths -- if there are a lot of long words or short words, probably need to be dropped.
  DTM_summarise <- DTM_terms %>%
    group_by(term) %>%
    summarise(
      n = n(),
      total = sum(count)) %>%
    mutate(nchar = nchar(term))
  
  ret_list[['DTM_summarise']]<-DTM_summarise

  
  
  if (tidyTrim) {
    # Words with too many characters are probably not actually words, and extremely uncommon words won’t help when searching for patterns.
    tidy_trim <- DTM_summarise %>%
      filter(
        nchar < 16,
        n > 1,
        total > 3) %>%
      select(term) %>%
      inner_join(DTM_terms)
    
    # Further steps in analyzing this “bag-of-words” require returning to the Document-Term-Matrix structure.
    DTM_trim <- tidy_trim %>%
      cast_dtm(document, term, count)
    
    ret_list[['DTM_trim']]<-DTM_trim
  }
  
  return(ret_list)
}

```

```{r}

#ndocs <- nrow(fulltext_df)
# we don't want to ignore extremely rare words for now, so setting this to infinity
#minTermFreq <- -Inf
# ignore overly common words i.e. terms that appear in more than 50% of the documents
#maxTermFreq <- ndocs * .5
# bounds = list(global = c(minTermFreq, maxTermFreq))
#  
#tokenize = NLP_tokenizer,

# stemming: dropping -ing -s -es
# lemmatization -- where is this?
# tm list of stopwords: https://rdrr.io/rforge/tm/man/stopwords.html
control_list_ngram = list(removePunctuation = T,
                          removeNumbers = T, 
                          stopwords = TRUE,
                          tolower = T, 
                          stemming = T,
                          wordLengths=c(3, 15)
                          )

```

# Document-Term Matrix

```{r}

# now have to make compatible with a VCorpus object & create a DTM

full_DocTermMatrix <- VectorSource(fulltext_df$agg_full_text) %>%
  VCorpus() %>%
  DocumentTermMatrix(control = control_list_ngram)

```

```{r}
inspect(full_DocTermMatrix)
```

```{r}

full_DTM<-tidy_words(full_DocTermMatrix, tidyTrim=TRUE)


```
```{r}

plotTitle = "Full Corpus"

png_fn = paste0("Images/WordLengthFreq_Hist.png",collapse=NULL)

# save the image in png format
png(png_fn, width=12, height=8, units="in", res=300)
ggplot(full_DTM[['DTM_summarise']], aes(x = nchar)) +
  ggtitle(plotTitle) +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_histogram(binwidth = 1)
dev.off()


```

# to save DTM as hard-copy csv
```{r}

write.csv((as.matrix(full_DocTermMatrix)), "Data/02_Working/full_DocTermMatrix.csv")
full_dtm_matrix = as.matrix(full_DocTermMatrix)
```

# Below is the list of most important terms for the Finding Dory movie as determined using tf-idf weighting.


```{r}
# [1] "Portland Press Herald; Portland, Me." [2] "Sun Journal; Lewiston, Me."           
# [3] "Kennebec Journal; Augusta, Me."      [4] "Morning Sentinel; Waterville, Me."    
# "Maine Times; Portland, Me."           "Bangor Daily News; Bangor, Me."  

pub_list <- c("PPH", "SJ", "KJ", "MS", "MT", "BDN")

for (i in 1:nrow(full_dtm_matrix)) {
  png_fn = paste0("Images/",i,"_",pub_list[i],"_wc.png",collapse=NULL)

  # save the image in png format
  png(png_fn, width=12, height=8, units="in", res=300)
  wordcloud(colnames(full_dtm_matrix), full_dtm_matrix[i, ], max.words = 200, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
  dev.off()

}
```



### Calculating distance

```{r}
distMatrix <- dist(full_dtm_matrix, method="euclidean")


```

```{r}
png_fn = paste0("Images/dmatrix_clusterDendogram.png",collapse=NULL)

# save the image in png format
png(png_fn, width=12, height=8, units="in", res=300)
groups <- hclust(distMatrix,method="ward.D")
plot(groups, cex=0.9, hang=-1, labels=pub_list)
rect.hclust(groups, k=5)
dev.off()


```

# Topic Modelling

## LDA
```{r}
library(topicmodels)

seed = 12345
fit = LDA(BDN_DocTermMatrix, k = 5, control = list(seed=seed))
```


```{r}
terms(fit, 20)

```


```{r}
email_topics <- as.data.frame(
  posterior(fit, dtm_trimmed)$topics)

```

```{r}
# console
head(email_topics)

```


```{r}
library(ggwordcloud)

topics <- tidy(fit) %>%
  filter(beta > 0.004)

ggplot(topics,
  aes(size = beta, label = term)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  facet_wrap(vars(topic))
```